{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e7ea1c",
   "metadata": {},
   "source": [
    "# Fine Tuning a model in 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d676c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1dbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a06e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b96734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956cabbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                instruction  \\\n",
      "0   ¿Qué significa la tipología Management?   \n",
      "1   ¿Qué significa la tipología Functional?   \n",
      "2    ¿Qué significa la tipología Technical?   \n",
      "3  ¿Qué significa la tipología Operational?   \n",
      "4  ¿Qué significa la tipología Maintenance?   \n",
      "5   ¿Qué significa la tipología Regulatory?   \n",
      "6         ¿Qué significa la tipología Info?   \n",
      "\n",
      "                                              output  \n",
      "0  La definición del tipo de requisito Management...  \n",
      "1  La definición del tipo de requisito Functional...  \n",
      "2  La definición del tipo de requisito Technical ...  \n",
      "3  La definición del tipo de requisito Operationa...  \n",
      "4  La definición del tipo de requisito Maintenanc...  \n",
      "5  La definición del tipo de requisito Regulatory...  \n",
      "6  La definición del tipo de requisito Info es   ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = './definitions.xlsx'\n",
    "\n",
    "# Leer el archivo Excel\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Crear las nuevas columnas\n",
    "df['instruction'] = df['Type'].apply(lambda x: f\"¿Qué significa la tipología {x}?\")\n",
    "df['output'] = df.apply(lambda row: f\"La definición del tipo de requisito {row['Type']} es {row['Definition']} y aquí tienes unos ejemplos. Ejemplo 1: {row['Example1']}. Ejemplo 2: {row['Example2']}\", axis=1)\n",
    "\n",
    "# Seleccionar las columnas relevantes\n",
    "result_df = df[['instruction', 'output']]\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(result_df)\n",
    "\n",
    "# Guardar el nuevo dataset en un archivo CSV\n",
    "result_df.to_csv('train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46127a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 12:10:23.387297: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcarmona\u001b[0m (\u001b[33mjcarmona-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import logging, pipeline\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Hugging Face secret key\n",
    "hugging_face_secret = os.getenv('HUGGING_FACE_SECRET')\n",
    "wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "# Login to wandb to log the metrics\n",
    "wandb.login(key = wandb_api_key)\n",
    "\n",
    "# Login to huggingface\n",
    "login(hugging_face_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60ba29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "PROJECT = \"Fine-Tuning CAF with Definitions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059f7fd",
   "metadata": {},
   "source": [
    "## Fine Tune in 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777338a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:55: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/Fine-Tuning CAF-TE/wandb/run-20240723_121028-xms0q9rm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcarmona-/Fine-Tuning%20CAF%20with%20Definitions/runs/xms0q9rm' target=\"_blank\">gallant-lake-5</a></strong> to <a href='https://wandb.ai/jcarmona-/Fine-Tuning%20CAF%20with%20Definitions' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcarmona-/Fine-Tuning%20CAF%20with%20Definitions' target=\"_blank\">https://wandb.ai/jcarmona-/Fine-Tuning%20CAF%20with%20Definitions</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcarmona-/Fine-Tuning%20CAF%20with%20Definitions/runs/xms0q9rm' target=\"_blank\">https://wandb.ai/jcarmona-/Fine-Tuning%20CAF%20with%20Definitions/runs/xms0q9rm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a26332868fa474189c2661b8402bee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097b0ace7279406f945e9c95e81d1877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c924a49a1754f868b5be79efdd0eef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4977 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 30/600 12:23 < 4:12:19, 0.04 it/s, Epoch 0.99/21]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.579900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from peft import LoraConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n",
    "    \"\"\"\n",
    "\n",
    "    local_rank: Optional[int] = field(default=-1, metadata={\"help\": \"Used for multi-gpu\"})\n",
    "\n",
    "    per_device_train_batch_size: Optional[int] = field(default=10)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=4)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=17)\n",
    "    learning_rate: Optional[float] = field(default=3e-5)\n",
    "    max_grad_norm: Optional[float] = field(default=1.0)\n",
    "    weight_decay: Optional[float] = field(default=0.01)\n",
    "    lora_alpha: Optional[int] = field(default=16)\n",
    "    lora_dropout: Optional[float] = field(default=0.1)\n",
    "    lora_r: Optional[int] = field(default=8)\n",
    "    max_seq_length: Optional[int] = field(default=256)\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        metadata={\n",
    "            \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"\n",
    "        }\n",
    "    )\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The preference dataset to use.\"},\n",
    "    )\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=\"train\",\n",
    "        metadata={\"help\": \"The local path to the dataset.\"},\n",
    "    )\n",
    "\n",
    "    use_4bit: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Activate 4bit precision base model loading\"},\n",
    "    )\n",
    "    use_nested_quant: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Activate nested quantization for 4bit base models\"},\n",
    "    )\n",
    "    bnb_4bit_compute_dtype: Optional[str] = field(\n",
    "        default=\"float16\",\n",
    "        metadata={\"help\": \"Compute dtype for 4bit base models\"},\n",
    "    )\n",
    "    bnb_4bit_quant_type: Optional[str] = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization type fp4 or nf4\"},\n",
    "    )\n",
    "    num_train_epochs: Optional[int] = field(\n",
    "        default=10,\n",
    "        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n",
    "    )\n",
    "    fp16: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enables fp16 training.\"},\n",
    "    )\n",
    "    bf16: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables bf16 training.\"},\n",
    "    )\n",
    "    packing: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Use packing dataset creating.\"},\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        default=\"adamw_torch\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: str = field(\n",
    "        default=\"cosine\",\n",
    "        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n",
    "    )\n",
    "    max_steps: int = field(default=600, metadata={\"help\": \"How many optimizer update steps to take\"})\n",
    "    warmup_steps: int = field(default=20, metadata={\"help\": \"# of steps to do a warmup for\"})\n",
    "    group_by_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Group sequences into batches with same length. Saves memory and speeds up training considerably.\"\n",
    "        },\n",
    "    )\n",
    "    save_steps: int = field(default=200, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    logging_steps: int = field(default=5, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    merge_and_push: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Merge and push weights after training\"},\n",
    "    )\n",
    "    output_dir: str = field(\n",
    "        default=\"./results_packing\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    report_to: Optional[str] = field(\n",
    "        default=\"wandb\",\n",
    "        metadata={\"help\": \"The integration to report the results and logs to.\"},\n",
    "    )\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "\n",
    "# Remove the Jupyter-specific arguments\n",
    "sys.argv = sys.argv[:1]\n",
    "script_args = parser.parse_args_into_dataclasses()[0]\n",
    "# Initialize wandb\n",
    "wandb.init(project=PROJECT, config=vars(script_args))\n",
    "\n",
    "\n",
    "def gen_batches_train():\n",
    "    \"\"\"\n",
    "    Generator function that yields batches of data for training.\n",
    "    \"\"\"\n",
    "    if script_args.dataset_path:\n",
    "        ds = load_from_disk(script_args.dataset_path)\n",
    "    else:\n",
    "        ds = load_dataset(script_args.dataset_name, streaming=True, split=\"train\")\n",
    "\n",
    "# Prompt example\n",
    "\n",
    "#         p = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "# Eres un asistente intelegente<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# hola<|eot_id|>\"\"\"\n",
    "    # Iterate over the dataset\n",
    "    for sample in iter(ds):\n",
    "        # Extract instruction and input from the sample\n",
    "        instruction = str(sample['instruction'])\n",
    "        out_text = str(sample['output'])\n",
    "        # Format the prompt\n",
    "        formatted_prompt = (\n",
    "            f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n Classify the sentence into: 'Info','Functional', 'Regulatory', 'Management', 'Operational', 'Technical' or 'Maintenance'. Sentence: \\n{instruction} \\n\\n### Response:\\n\"\n",
    "            f\"<|eot_id|><|start_header_id|>asssitant<|end_header_id|>\\n\\n\",\n",
    "            f\"{str(out_text)}\"\n",
    "            f\"<|eot_id|><|end_of_text|>\"\n",
    "        )\n",
    "        formatted_prompt = \"\".join(formatted_prompt)\n",
    "        yield {'text': formatted_prompt}\n",
    "\n",
    "def create_and_prepare_model(args):\n",
    "    \"\"\" Create and prepare the model for training.\n",
    "\n",
    "    Args:\n",
    "        args: Arguments for the model.\n",
    "\n",
    "    Returns:\n",
    "        model: The model to train.\n",
    "        peft_config: The configuration for the PEFT model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "    \"\"\"\n",
    "    # Load the model with the specified configuration for quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        load_in_4bit=args.use_4bit,\n",
    "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=getattr(torch, args.bnb_4bit_compute_dtype),\n",
    "        bnb_4bit_use_double_quant=args.use_nested_quant,\n",
    "    )\n",
    "\n",
    "    # Load the entire model on the GPU 0\n",
    "    # switch to `device_map = \"auto\"` for multi-GPU\n",
    "    device_map = {\"\": 0}\n",
    "\n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name, \n",
    "        quantization_config=bnb_config, \n",
    "        device_map=device_map, \n",
    "        use_auth_token=True,\n",
    "    )\n",
    "    \n",
    "    # Set the configuration for the PEFT model\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=script_args.lora_alpha,\n",
    "        lora_dropout=script_args.lora_dropout,\n",
    "        r=script_args.lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        target_modules=['q_proj', 'v_proj'],\n",
    "    )\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, peft_config, tokenizer\n",
    "\n",
    "# Set up the training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=script_args.output_dir, # The output directory\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size, # The batch size per GPU\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps, # The number of gradient accumulation steps\n",
    "    optim=script_args.optim, # The optimizer to use\n",
    "    save_steps=script_args.save_steps, # Save a checkpoint every X updates steps\n",
    "    logging_steps=script_args.logging_steps, # Log every X updates steps\n",
    "    learning_rate=script_args.learning_rate, # The learning rate\n",
    "    fp16=script_args.fp16, # Enable fp16 training\n",
    "    bf16=script_args.bf16, # Enable bf16 training\n",
    "    max_grad_norm=script_args.max_grad_norm, # The maximum gradient norm\n",
    "    max_steps=script_args.max_steps, # The maximum number of optimizer update steps\n",
    "    warmup_steps=script_args.warmup_steps, # The number of steps to do a warmup for\n",
    "    group_by_length=script_args.group_by_length, # Group sequences into batches with same length\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type, # The learning rate schedule\n",
    "    report_to=script_args.report_to, # The integration to report the results and logs to\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing, # Enable gradient checkpointing\n",
    ")\n",
    "\n",
    "# Set up the model, PEFT configuration, and tokenizer\n",
    "model, peft_config, tokenizer = create_and_prepare_model(script_args)\n",
    "# Create the training generator\n",
    "train_gen = Dataset.from_generator(gen_batches_train)\n",
    "# Set the padding side\n",
    "tokenizer.padding_side = \"right\"\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_gen,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=script_args.max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=script_args.packing,\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Save the model\n",
    "if script_args.merge_and_push:\n",
    "    # Save the final model\n",
    "    output_dir = os.path.join(script_args.output_dir, \"final_checkpoints\")\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load the model for merging\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    model = model.merge_and_unload()\n",
    "    # Save the merged model\n",
    "    output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\n",
    "    model.save_pretrained(output_merged_dir, safe_serialization=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7756c",
   "metadata": {},
   "source": [
    "### Prompt example and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddeb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Eres un asistente intelegente<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "hola<|eot_id|>\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa432b",
   "metadata": {},
   "source": [
    "Function to make inference of the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90114924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):  \n",
    "    \"\"\" LLM inference function to give response to the user prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The response to the user prompt.\n",
    "    \"\"\"\n",
    "    p = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n Classify the sentence into: 'Info','Functional', 'Regulatory', 'Management', 'Operational', 'Technical' or 'Maintenance'. Sentence: \\n{prompt} \\n\\n### Response:\\n\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "    marcador = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    \n",
    "    # Encontrar la posición del marcador en el texto\n",
    "    \n",
    "\n",
    "    \n",
    "    # Devolver el contenido después del marcador\n",
    "    result = pipe(p)\n",
    "    result = result[0]['generated_text']\n",
    "    posicion = result.find(marcador)\n",
    "    return result[posicion + len(marcador)+2:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm(\"IT-systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3aa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917d9a3",
   "metadata": {},
   "source": [
    "# Prueba de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d120580",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "for i in range(10):\n",
    "    print(\"Instruction: \", test['instruction'][i], \"\\nOutput: \", test['output'][i], \"\\nInference Output: \", llm(test['instruction'][i]))\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8e339",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def test():\n",
    "    \"\"\" Test the model on the test dataset.\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    results = test_df['output']\n",
    "    text = test_df['instruction']\n",
    "    resultados = []\n",
    "    \n",
    "    for i in tqdm(text, desc=\"Processing\"):\n",
    "        response = llm(i)\n",
    "        res = response\n",
    "        resultados.append(res)\n",
    "    \n",
    "    # Filtrar resultados que son None\n",
    "    filtered_results = [res for res in resultados if res is not None]\n",
    "    filtered_actual = [res for i, res in enumerate(results.tolist()) if resultados[i] is not None]\n",
    "    \n",
    "    print(filtered_results)\n",
    "    print(filtered_actual)  # Convierte la serie de pandas a una lista para que se vea igual que resultados\n",
    "    \n",
    "    # Compute accuracy and F1 score\n",
    "    accuracy = accuracy_score(filtered_actual, filtered_results)\n",
    "    f1 = f1_score(filtered_actual, filtered_results, average='weighted')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4f142",
   "metadata": {},
   "source": [
    "# Prompting vs Fine-Tuning with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd171c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def original_model(text_input, endpoint_url:str = \"http://172.16.59.1:8000/v2/models/ensemble/generate\"):\n",
    "    \"\"\" Function to generate text using the original model.\n",
    "\n",
    "    Args:\n",
    "        text_input (str): The input text.\n",
    "        endpoint_url (str, optional): Endpoint URL for the model. Defaults to \"http://172.16.59.1:8000/v2/models/ensemble/generate\".\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    p = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Below is an instruction that describes a task. Answer with an unique word.\\n\\n### Instruction:\\n Classify the sentence into: 'Info','Functional', 'Regulatory', 'Management', 'Operational', 'Technical' or 'Maintenance'. Sentence: \\n{text_input} \\n\\n### Response:\\n\"\n",
    "        f\"<|eot_id|><|start_header_id|>asssitant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    payload = {\n",
    "        \"text_input\": p,\n",
    "        \"parameters\": {\n",
    "            \"max_tokens\": 1024,\n",
    "            \"bad_words\": [\"\"],\n",
    "            \"stop_words\": [\"\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(endpoint_url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"text_output\"]\n",
    "    else:\n",
    "        response.raise_for_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79aa913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompting():\n",
    "    \"\"\"Function to generate text using the original model.\"\"\"    \n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    results = test_df['output']\n",
    "    text = test_df['instruction']\n",
    "    resultados = []\n",
    "    \n",
    "    for i in tqdm(text, desc=\"Processing\"):\n",
    "        response = original_model(i)\n",
    "        res = response\n",
    "        resultados.append(res)\n",
    "    \n",
    "    # Filtrar resultados que son None\n",
    "    filtered_results = [res for res in resultados if res is not None]\n",
    "    filtered_actual = [res for i, res in enumerate(results.tolist()) if resultados[i] is not None]\n",
    "    \n",
    "#     print(filtered_results)\n",
    "    \n",
    "    # Compute accuracy and F1 score\n",
    "    accuracy = accuracy_score(filtered_actual, filtered_results)\n",
    "    f1 = f1_score(filtered_actual, filtered_results, average='weighted')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    return filtered_results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa69e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = prompting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23accc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "for i in range(10):\n",
    "    print(\"Instruction: \", test['instruction'][i], \"\\nOutput: \", test['output'][i], \"\\nInference Output: \", original_model(test['instruction'][i]))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
