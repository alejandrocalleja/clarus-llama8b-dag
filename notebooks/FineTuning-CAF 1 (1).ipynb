{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4fe478",
   "metadata": {},
   "source": [
    "# Fine Tuning a model in 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54851d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 06:04:34.190222: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcarmona\u001b[0m (\u001b[33mjcarmona-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import logging, pipeline\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the Hugging Face secret key\n",
    "hugging_face_secret = os.getenv('HUGGING_FACE_SECRET')\n",
    "wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "# Login to wandb to log the metrics\n",
    "wandb.login(key = wandb_api_key)\n",
    "\n",
    "# Login to huggingface\n",
    "login(hugging_face_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24526e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "PROJECT = \"Fine-Tuning CAF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e7679f",
   "metadata": {},
   "source": [
    "## Fine Tune in 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc593bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/Fine-Tuning CAF-TE/wandb/run-20240718_180041-kpx0tlum</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcarmona-/Fine-Tuning%20CAF/runs/kpx0tlum' target=\"_blank\">light-night-3</a></strong> to <a href='https://wandb.ai/jcarmona-/Fine-Tuning%20CAF' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcarmona-/Fine-Tuning%20CAF' target=\"_blank\">https://wandb.ai/jcarmona-/Fine-Tuning%20CAF</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcarmona-/Fine-Tuning%20CAF/runs/kpx0tlum' target=\"_blank\">https://wandb.ai/jcarmona-/Fine-Tuning%20CAF/runs/kpx0tlum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72111ed20814a9ba6d181760334bfef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='144' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/200 1:04:33 < 25:27, 0.04 it/s, Epoch 4.88/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.580200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>4.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.846900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.594800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.884100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.251100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from peft import LoraConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n",
    "    \"\"\"\n",
    "\n",
    "    local_rank: Optional[int] = field(default=-1, metadata={\"help\": \"Used for multi-gpu\"})\n",
    "\n",
    "    per_device_train_batch_size: Optional[int] = field(default=10)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=4)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=17)\n",
    "    learning_rate: Optional[float] = field(default=3e-5)\n",
    "    max_grad_norm: Optional[float] = field(default=1.0)\n",
    "    weight_decay: Optional[float] = field(default=0.01)\n",
    "    lora_alpha: Optional[int] = field(default=16)\n",
    "    lora_dropout: Optional[float] = field(default=0.1)\n",
    "    lora_r: Optional[int] = field(default=8)\n",
    "    max_seq_length: Optional[int] = field(default=256)\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        metadata={\n",
    "            \"help\": \"The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.\"\n",
    "        }\n",
    "    )\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The preference dataset to use.\"},\n",
    "    )\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=\"train\",\n",
    "        metadata={\"help\": \"The local path to the dataset.\"},\n",
    "    )\n",
    "\n",
    "    use_4bit: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Activate 4bit precision base model loading\"},\n",
    "    )\n",
    "    use_nested_quant: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Activate nested quantization for 4bit base models\"},\n",
    "    )\n",
    "    bnb_4bit_compute_dtype: Optional[str] = field(\n",
    "        default=\"float16\",\n",
    "        metadata={\"help\": \"Compute dtype for 4bit base models\"},\n",
    "    )\n",
    "    bnb_4bit_quant_type: Optional[str] = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization type fp4 or nf4\"},\n",
    "    )\n",
    "    num_train_epochs: Optional[int] = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"The number of training epochs for the reward model.\"},\n",
    "    )\n",
    "    fp16: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enables fp16 training.\"},\n",
    "    )\n",
    "    bf16: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables bf16 training.\"},\n",
    "    )\n",
    "    packing: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Use packing dataset creating.\"},\n",
    "    )\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Enables gradient checkpointing.\"},\n",
    "    )\n",
    "    optim: Optional[str] = field(\n",
    "        default=\"adamw_torch\",\n",
    "        metadata={\"help\": \"The optimizer to use.\"},\n",
    "    )\n",
    "    lr_scheduler_type: str = field(\n",
    "        default=\"cosine\",\n",
    "        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n",
    "    )\n",
    "    max_steps: int = field(default=200, metadata={\"help\": \"How many optimizer update steps to take\"})\n",
    "    warmup_steps: int = field(default=20, metadata={\"help\": \"# of steps to do a warmup for\"})\n",
    "    group_by_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Group sequences into batches with same length. Saves memory and speeds up training considerably.\"\n",
    "        },\n",
    "    )\n",
    "    save_steps: int = field(default=200, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    logging_steps: int = field(default=5, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    merge_and_push: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Merge and push weights after training\"},\n",
    "    )\n",
    "    output_dir: str = field(\n",
    "        default=\"./results_packing\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    report_to: Optional[str] = field(\n",
    "        default=\"wandb\",\n",
    "        metadata={\"help\": \"The integration to report the results and logs to.\"},\n",
    "    )\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "\n",
    "# Remove the Jupyter-specific arguments\n",
    "sys.argv = sys.argv[:1]\n",
    "script_args = parser.parse_args_into_dataclasses()[0]\n",
    "# Initialize wandb\n",
    "wandb.init(project=PROJECT, config=vars(script_args))\n",
    "\n",
    "\n",
    "def gen_batches_train():\n",
    "    \"\"\"\n",
    "    Generator function that yields batches of data for training.\n",
    "    \"\"\"\n",
    "    if script_args.dataset_path:\n",
    "        ds = load_from_disk(script_args.dataset_path)\n",
    "    else:\n",
    "        ds = load_dataset(script_args.dataset_name, streaming=True, split=\"train\")\n",
    "\n",
    "# Prompt example\n",
    "\n",
    "#         p = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "# Eres un asistente intelegente<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# hola<|eot_id|>\"\"\"\n",
    "    # Iterate over the dataset\n",
    "    for sample in iter(ds):\n",
    "        # Extract instruction and input from the sample\n",
    "        instruction = str(sample['instruction'])\n",
    "        out_text = str(sample['output'])\n",
    "        # Format the prompt\n",
    "        formatted_prompt = (\n",
    "            f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n Classify the sentence into: 'Info','Functional', 'Regulatory', 'Management', 'Operational', 'Technical' or 'Maintenance'. Sentence: \\n{instruction} \\n\\n### Response:\\n\"\n",
    "            f\"<|eot_id|><|start_header_id|>asssitant<|end_header_id|>\\n\\n\",\n",
    "            f\"{str(out_text)}\"\n",
    "            f\"<|eot_id|><|end_of_text|>\"\n",
    "        )\n",
    "        formatted_prompt = \"\".join(formatted_prompt)\n",
    "        yield {'text': formatted_prompt}\n",
    "\n",
    "def create_and_prepare_model(args):\n",
    "    \"\"\" Create and prepare the model for training.\n",
    "\n",
    "    Args:\n",
    "        args: Arguments for the model.\n",
    "\n",
    "    Returns:\n",
    "        model: The model to train.\n",
    "        peft_config: The configuration for the PEFT model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "    \"\"\"\n",
    "    # Load the model with the specified configuration for quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        load_in_4bit=args.use_4bit,\n",
    "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=getattr(torch, args.bnb_4bit_compute_dtype),\n",
    "        bnb_4bit_use_double_quant=args.use_nested_quant,\n",
    "    )\n",
    "\n",
    "    # Load the entire model on the GPU 0\n",
    "    # switch to `device_map = \"auto\"` for multi-GPU\n",
    "    device_map = {\"\": 0}\n",
    "\n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name, \n",
    "        quantization_config=bnb_config, \n",
    "        device_map=device_map, \n",
    "        use_auth_token=True,\n",
    "    )\n",
    "    \n",
    "    # Set the configuration for the PEFT model\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=script_args.lora_alpha,\n",
    "        lora_dropout=script_args.lora_dropout,\n",
    "        r=script_args.lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        target_modules=['q_proj', 'v_proj'],\n",
    "    )\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, peft_config, tokenizer\n",
    "\n",
    "# Set up the training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=script_args.output_dir, # The output directory\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size, # The batch size per GPU\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps, # The number of gradient accumulation steps\n",
    "    optim=script_args.optim, # The optimizer to use\n",
    "    save_steps=script_args.save_steps, # Save a checkpoint every X updates steps\n",
    "    logging_steps=script_args.logging_steps, # Log every X updates steps\n",
    "    learning_rate=script_args.learning_rate, # The learning rate\n",
    "    fp16=script_args.fp16, # Enable fp16 training\n",
    "    bf16=script_args.bf16, # Enable bf16 training\n",
    "    max_grad_norm=script_args.max_grad_norm, # The maximum gradient norm\n",
    "    max_steps=script_args.max_steps, # The maximum number of optimizer update steps\n",
    "    warmup_steps=script_args.warmup_steps, # The number of steps to do a warmup for\n",
    "    group_by_length=script_args.group_by_length, # Group sequences into batches with same length\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type, # The learning rate schedule\n",
    "    report_to=script_args.report_to, # The integration to report the results and logs to\n",
    "    gradient_checkpointing=script_args.gradient_checkpointing, # Enable gradient checkpointing\n",
    ")\n",
    "\n",
    "# Set up the model, PEFT configuration, and tokenizer\n",
    "model, peft_config, tokenizer = create_and_prepare_model(script_args)\n",
    "# Create the training generator\n",
    "train_gen = Dataset.from_generator(gen_batches_train)\n",
    "# Set the padding side\n",
    "tokenizer.padding_side = \"right\"\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_gen,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=script_args.max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=script_args.packing,\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Save the model\n",
    "if script_args.merge_and_push:\n",
    "    # Save the final model\n",
    "    output_dir = os.path.join(script_args.output_dir, \"final_checkpoints\")\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load the model for merging\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    model = model.merge_and_unload()\n",
    "    # Save the merged model\n",
    "    output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\n",
    "    model.save_pretrained(output_merged_dir, safe_serialization=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73f4a4",
   "metadata": {},
   "source": [
    "### Prompt example and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "282f26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Eres un asistente intelegente<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "hola<|eot_id|>\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc2c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41b01d",
   "metadata": {},
   "source": [
    "Function to make inference of the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):  \n",
    "    \"\"\" LLM inference function to give response to the user prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The response to the user prompt.\n",
    "    \"\"\"\n",
    "    p = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n Classify the sentence into: 'Info','Functional', 'Regulatory', 'Management', 'Operational', 'Technical' or 'Maintenance'. Sentence: \\n{prompt} \\n\\n### Response:\\n\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "    marcador = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    \n",
    "    # Encontrar la posición del marcador en el texto\n",
    "    \n",
    "\n",
    "    \n",
    "    # Devolver el contenido después del marcador\n",
    "    result = pipe(p)\n",
    "    result = result[0]['generated_text']\n",
    "    posicion = result.find(marcador)\n",
    "    return result[posicion + len(marcador)+2:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cfa8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm(\"IT-systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "674659e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Management\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda1973",
   "metadata": {},
   "source": [
    "# Prueba de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79dfb7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:  Systems Test Completion Reviews (TCRs): \n",
      "Output:  Info \n",
      "Inference Output:  Management\n",
      "-----------------------------------------------\n",
      "Instruction:  Without derogating from the Project Company’s obligations under and pursuant to the Agreement, the Project Company shall ensure that: \n",
      "Output:  Management \n",
      "Inference Output:  Management\n",
      "-----------------------------------------------\n",
      "Instruction:  Additional and/or alternative control measures and actions shall be implemented as are necessary and required to limit dust, prevent air pollution and preserve the quality of the air. \n",
      "Output:  Technical \n",
      "Inference Output:  Operational\n",
      "-----------------------------------------------\n",
      "Instruction:  The Project Company shall submit to the Owner a SDP as part of the SRR. \n",
      "Output:  Management \n",
      "Inference Output:  Technical\n",
      "-----------------------------------------------\n",
      "Instruction:  Reflect and conform to: (i) the Project Timeline and Milestones stipulated in Appendix B to the Agreement and (ii) the Project Schedule stipulated in Appendix C to the Agreement, including (without limitation) the Construction Period stipulated therein. \n",
      "Output:  Management \n",
      "Inference Output:  Technical\n",
      "-----------------------------------------------\n",
      "Instruction:  RSS communication to LRT System Rolling Stock shall be via the DMRS / WWRS to the OB-S&TC ATS system and TDD. \n",
      "Output:  Functional \n",
      "Inference Output:  Management\n",
      "-----------------------------------------------\n",
      "Instruction:  Fiber-optic Vehicle joint coupler insertion loss shall be &#8804;0.2dB. \n",
      "Output:  Technical \n",
      "Inference Output:  Technical\n",
      "-----------------------------------------------\n",
      "Instruction:  Credit card Loading Machines (CLM) \n",
      "Output:  Maintenance \n",
      "Inference Output:  Management\n",
      "-----------------------------------------------\n",
      "Instruction:  Interface with the S&TC for automatic download of the daily operating schedule performed by the S&TC before start of revenue service. \n",
      "Output:  Functional \n",
      "Inference Output:  Technical\n",
      "-----------------------------------------------\n",
      "Instruction:  DMRS, WWRS, EARS, cellular coverage, SNIR and RSSI tests for each of the onboard radio communication systems inside the Vehicle and the driver's cab, with doors and windows closed while the Vehicle is on the move with various speeds along the entire LRT Alignment/ROW. \n",
      "Output:  Technical \n",
      "Inference Output:  Technical\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "for i in range(10):\n",
    "    print(\"Instruction: \", test['instruction'][i], \"\\nOutput: \", test['output'][i], \"\\nInference Output: \", llm(test['instruction'][i]))\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8910e14",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c995afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/102 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 102/102 [00:07<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Management', 'Management', 'Management', 'Management', 'Management', 'Management', 'Management', 'Management', 'Technical', 'Technical', 'Management', 'Management', 'Technical', 'Management', 'Management', 'Management', 'Management', 'Technical', 'Technical', 'Management', 'Info', 'Info', 'Management', 'Management', 'Management', 'Management', 'Technical', 'Technical', 'Management', 'Info', 'Technical', 'Technical', 'Management', 'Info', 'Info', 'Management', 'Management', 'Management', 'Info', 'Technical', 'Management', 'Technical', 'Technical', 'Functional', 'Management', 'Management', 'Technical', 'Management', 'Management', 'Info', 'Management', 'Management', 'Technical', 'Management', 'Management', 'Technical', 'Management', 'Management', 'Management', 'Technical', 'Management', 'Management', 'Management', 'Info', 'Management', 'Management', 'Technical', 'Technical', 'Management', 'Technical', 'Management', 'Management', 'Info', 'Management', 'Management', 'Technical', 'Management', 'Technical', 'Management', 'Technical', 'Management', 'Management', 'Management', 'Management', 'Technical', 'Management', 'Management', 'Technical', 'Management', 'Technical', 'Management', 'Technical', 'Management', 'Management', 'Technical', 'Maintenance', 'Info', 'Info', 'Regulatory', 'Technical', 'Info', 'Technical']\n",
      "['Management', 'Info', 'Technical', 'Management', 'Technical', 'Management', 'Management', 'Management', 'Management', 'Maintenance', 'Management', 'Management', 'Management', 'Functional', 'Functional', 'Management', 'Functional', 'Maintenance', 'Technical', 'Info', 'Maintenance', 'Info', 'Management', 'Management', 'Technical', 'Management', 'Management', 'Management', 'Management', 'Info', 'Management', 'Management', 'Info', 'Info', 'Functional', 'Management', 'Functional', 'Management', 'Management', 'Technical', 'Management', 'Technical', 'Management', 'Info', 'Management', 'Management', 'Technical', 'Info', 'Management', 'Maintenance', 'Management', 'Technical', 'Technical', 'Management', 'Technical', 'Management', 'Management', 'Management', 'Management', 'Management', 'Management', 'Management', 'Management', 'Info', 'Management', 'Management', 'Technical', 'Management', 'Technical', 'Management', 'Management', 'Management', 'Technical', 'Functional', 'Management', 'Technical', 'Info', 'Technical', 'Functional', 'Functional', 'Management', 'Management', 'Technical', 'Technical', 'Technical', 'Info', 'Regulatory', 'Technical', 'Management', 'Technical', 'Management', 'Management', 'Technical', 'Management', 'Regulatory', 'Management', 'Technical', 'Functional', 'Management', 'Technical', 'Info', 'Functional']\n",
      "Accuracy: 0.5196078431372549\n",
      "F1 Score: 0.48336723539098064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def test():\n",
    "    \"\"\" Test the model on the test dataset.\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    results = test_df['output']\n",
    "    text = test_df['instruction']\n",
    "    resultados = []\n",
    "    \n",
    "    for i in tqdm(text, desc=\"Processing\"):\n",
    "        response = llm(i)\n",
    "        res = response\n",
    "        resultados.append(res)\n",
    "    \n",
    "    # Filtrar resultados que son None\n",
    "    filtered_results = [res for res in resultados if res is not None]\n",
    "    filtered_actual = [res for i, res in enumerate(results.tolist()) if resultados[i] is not None]\n",
    "    \n",
    "    print(filtered_results)\n",
    "    print(filtered_actual)  # Convierte la serie de pandas a una lista para que se vea igual que resultados\n",
    "    \n",
    "    # Compute accuracy and F1 score\n",
    "    accuracy = accuracy_score(filtered_actual, filtered_results)\n",
    "    f1 = f1_score(filtered_actual, filtered_results, average='weighted')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ff4b3",
   "metadata": {},
   "source": [
    "# Prompting vs Fine-Tuning with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614f2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def original_model(text_input, endpoint_url:str = \"http://172.16.59.1:8000/v2/models/ensemble/generate\"):\n",
    "    \"\"\" Function to generate text using the original model.\n",
    "\n",
    "    Args:\n",
    "        text_input (str): The input text.\n",
    "        endpoint_url (str, optional): Endpoint URL for the model. Defaults to \"http://172.16.59.1:8000/v2/models/ensemble/generate\".\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    p = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Below is an instruction that describes a task. Answer with an unique word.\\n\\n### Instruction:\\n Classify the sentence into: 'Info','Functional', 'Regulatory', 'Management', 'Operational', 'Technical' or 'Maintenance'. Sentence: \\n{text_input} \\n\\n### Response:\\n\"\n",
    "        f\"<|eot_id|><|start_header_id|>asssitant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    payload = {\n",
    "        \"text_input\": p,\n",
    "        \"parameters\": {\n",
    "            \"max_tokens\": 1024,\n",
    "            \"bad_words\": [\"\"],\n",
    "            \"stop_words\": [\"\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(endpoint_url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"text_output\"]\n",
    "    else:\n",
    "        response.raise_for_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5030d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompting():\n",
    "    \"\"\"Function to generate text using the original model.\"\"\"    \n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    results = test_df['output']\n",
    "    text = test_df['instruction']\n",
    "    resultados = []\n",
    "    \n",
    "    for i in tqdm(text, desc=\"Processing\"):\n",
    "        response = original_model(i)\n",
    "        res = response\n",
    "        resultados.append(res)\n",
    "    \n",
    "    # Filtrar resultados que son None\n",
    "    filtered_results = [res for res in resultados if res is not None]\n",
    "    filtered_actual = [res for i, res in enumerate(results.tolist()) if resultados[i] is not None]\n",
    "    \n",
    "#     print(filtered_results)\n",
    "    \n",
    "    # Compute accuracy and F1 score\n",
    "    accuracy = accuracy_score(filtered_actual, filtered_results)\n",
    "    f1 = f1_score(filtered_actual, filtered_results, average='weighted')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    return filtered_results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da208314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 102/102 [00:20<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = prompting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e55010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:  All training courses shall be presented in English (except where the training instructor and all trainees are Hebrew speakers) by a member of the Project Company’s training staff. \n",
      "Output:  Management \n",
      "Inference Output:  **Unique Word:** **Categorization**\n",
      "\n",
      "Instruction:  The Uninterruptible Power System (UPS) shall comply with the following input requirements; \n",
      "Output:  Info \n",
      "Inference Output:  **Classification:** Technical\n",
      "\n",
      "Instruction:  Secure communication protocol for remote access administration (e.g. SSH), and SNMP v3. \n",
      "Output:  Technical \n",
      "Inference Output:  **Authentication**\n",
      "\n",
      "Instruction:  Time synchronization; and \n",
      "Output:  Management \n",
      "Inference Output:  **Classification:** Technical\n",
      "\n",
      "Instruction:  Occupational hygiene; \n",
      "Output:  Technical \n",
      "Inference Output:  **Hygienic**\n",
      "\n",
      "Instruction:  The Project Company shall inform the Owner at least forty-eight (48) hours prior to the performance of each Assessment Point. \n",
      "Output:  Management \n",
      "Inference Output:  **Unique Word:** Notification\n",
      "\n",
      "Instruction:  In order, inter alia, to ascertain compliance with the Remaining Service Life Targets prior to the Handback Date, the Project Company shall perform two (2) inspections of the LRT System, which shall consist of the examination, and, where the Owner deems it necessary, the testing of all LRT System assets (hereinafter the “Pre-Handback Inspections”). \n",
      "Output:  Management \n",
      "Inference Output:  **Unique Word:** **Regulatory**\n",
      "\n",
      "Instruction:  Documentation contained in the Project Documentation File shall be organized and arranged in a hierarchical, searchable, easy-to-read and easily accessible manner and format. \n",
      "Output:  Management \n",
      "Inference Output:  **Hierarchical**\n",
      "\n",
      "Instruction:  Maintenance performance and responsibility including completion of incident report forms; \n",
      "Output:  Management \n",
      "Inference Output:  **Regulatory**\n",
      "\n",
      "Instruction:  Earthing pits at platforms \n",
      "Output:  Maintenance \n",
      "Inference Output:  **Technical**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "for i in range(10):\n",
    "    print(\"Instruction: \", test['instruction'][i], \"\\nOutput: \", test['output'][i], \"\\nInference Output: \", original_model(test['instruction'][i]))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
